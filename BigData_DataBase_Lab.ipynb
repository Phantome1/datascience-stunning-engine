{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22865edd",
   "metadata": {},
   "source": [
    "# Installation Tutorial &  Lab\n",
    "## Dr. Aurelle TCHAGNA\n",
    "# Course: Advanced AI / Big Data Lab (Landmark University, M.Tech)  \n",
    "**Goal:** Install and validate **Apache Spark**, **PySpark**, **MongoDB**, **PyMongo**, **MongoDB Spark Connector**, and **MongoDB Compass**, then practice PySpark with a **WordCount** pipeline and DataFrame analytics.\n",
    "\n",
    "This notebook contains:  \n",
    "1 installation steps (Windows/Linux/macOS)  \n",
    "2 verification commands  \n",
    "3 PySpark lab (RDD + DataFrame)  \n",
    "4 MongoDB operations (PyMongo + Spark Connector config)  \n",
    "5 exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a748b",
   "metadata": {},
   "source": [
    "## 0) What you will install\n",
    "### Required\n",
    "- **Java (JDK 11 or 17)**: Spark runs on the JVM.\n",
    "- **Apache Spark**: distributed compute engine.\n",
    "- **Python 3.9+**: for PySpark scripts and notebooks.\n",
    "- **PySpark**: Python API for Spark.\n",
    "- **MongoDB Community Server**: NoSQL database.\n",
    "- **PyMongo**: Python driver for MongoDB.\n",
    "- **MongoDB Compass**: GUI to view/edit MongoDB data.\n",
    "\n",
    "### Optional (Recommended)\n",
    "- **MongoDB Spark Connector**: enables Spark to read/write MongoDB efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073dd08",
   "metadata": {},
   "source": [
    "## 1) Install Java (JDK)\n",
    "### Windows\n",
    "1. Install **JDK 17** (or 11).\n",
    "2. Set environment variables:\n",
    "   - `JAVA_HOME=C:\\Program Files\\Java\\jdk-17`\n",
    "   - Add `%JAVA_HOME%\\bin` to `PATH`\n",
    "\n",
    "### Ubuntu/Debian\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install -y openjdk-17-jdk\n",
    "java -version\n",
    "```\n",
    "\n",
    "### macOS (Homebrew)\n",
    "```bash\n",
    "brew install openjdk@17\n",
    "java -version\n",
    "```\n",
    "\n",
    "✅ **Check**\n",
    "```bash\n",
    "java -version\n",
    "echo %JAVA_HOME%   # Windows\n",
    "echo $JAVA_HOME    # Linux/macOS\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470b4b3",
   "metadata": {},
   "source": [
    "## 2) Install Apache Spark\n",
    "Download Spark (prebuilt for Hadoop) from the official Apache Spark site.\n",
    "\n",
    "After extracting, set:\n",
    "- `SPARK_HOME` to the Spark folder\n",
    "- Add `$SPARK_HOME/bin` to `PATH`\n",
    "\n",
    "**Windows example**\n",
    "- `SPARK_HOME=C:\\spark\\spark-3.5.1-bin-hadoop3`\n",
    "- Add `C:\\spark\\spark-3.5.1-bin-hadoop3\\bin` to PATH\n",
    "\n",
    "**Linux/macOS example**\n",
    "```bash\n",
    "export SPARK_HOME=$HOME/spark/spark-3.5.1-bin-hadoop3\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "✅ **Check**\n",
    "```bash\n",
    "spark-submit --version\n",
    "pyspark --version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2651a6",
   "metadata": {},
   "source": [
    "## 3) Install Python packages (PySpark + PyMongo)\n",
    "Create a virtual environment (recommended) then install packages.\n",
    "\n",
    "### Windows (PowerShell)\n",
    "```powershell\n",
    "python -m venv .venv\n",
    ".venv\\Scripts\\activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark pymongo\n",
    "```\n",
    "\n",
    "### Linux/macOS\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark pymongo\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93be88",
   "metadata": {},
   "source": [
    "## 4) Install MongoDB Community Server + Compass\n",
    "### MongoDB Server\n",
    "Install MongoDB Community Server (Windows/macOS/Linux) from MongoDB official downloads.\n",
    "\n",
    "✅ After installation, ensure MongoDB is running.\n",
    "\n",
    "**Windows**\n",
    "- Open **Services** → start **MongoDB Server**  \n",
    "- Default URI: `mongodb://localhost:27017`\n",
    "\n",
    "**Linux (systemd)**\n",
    "```bash\n",
    "sudo systemctl start mongod\n",
    "sudo systemctl status mongod\n",
    "```\n",
    "\n",
    "### MongoDB Compass\n",
    "Install **MongoDB Compass** and connect using:\n",
    "- `mongodb://localhost:27017`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583675a",
   "metadata": {},
   "source": [
    "## 5) MongoDB Spark Connector (important notes)\n",
    "Spark needs the MongoDB connector **JAR** when reading/writing MongoDB via Spark.\n",
    "\n",
    "### Option A: Use Maven coordinate (easy)\n",
    "In SparkSession config:\n",
    "- `org.mongodb.spark:mongo-spark-connector_2.12:10.3.0` (example)\n",
    "\n",
    "### Option B: Download connector JAR manually\n",
    "Provide it to Spark using `--jars` or `spark.jars`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febedf3",
   "metadata": {},
   "source": [
    "## 6) Quick verification inside Jupyter\n",
    "Run the next cells. They check PySpark/PyMongo and start a local Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5f0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.14.2 (tags/v3.14.2:df79316, Dec  5 2025, 17:18:21) [MSC v.1944 64 bit (AMD64)]\n",
      "PySpark version: 4.1.1\n",
      "PyMongo version: 4.16.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"PySpark version:\", pyspark.__version__)\n",
    "except Exception as e:\n",
    "    print(\"PySpark error:\", e)\n",
    "\n",
    "try:\n",
    "    import pymongo\n",
    "    print(\"PyMongo version:\", pymongo.__version__)\n",
    "except Exception as e:\n",
    "    print(\"PyMongo error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88731ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MongoSparkSession\")\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "    # Local MongoDB Spark Connector JAR\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        r\"C:\\spark-jars\\mongo-spark-connector_2.12-10.3.0.jar\"\n",
    "    )\n",
    "\n",
    "    # MongoDB connection\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017\")\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017\")\n",
    "\n",
    "    # Python path\n",
    "    .config(\"spark.pyspark.python\", os.environ.get(\"PYSPARK_PYTHON\"))\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afa05c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433d53c",
   "metadata": {},
   "source": [
    "# Part A — PySpark Lab (RDD): WordCount \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b5297",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"Spark is fast. Spark is general-purpose.\",\n",
    "    \"PySpark lets you use Spark with Python.\",\n",
    "    \"Big data processing with Spark is scalable and efficient.\",\n",
    "    \"MongoDB is a NoSQL database. PyMongo connects Python to MongoDB.\"\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(text)\n",
    "rdd.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55258d59",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(line: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", line.lower())\n",
    "\n",
    "word_counts = (\n",
    "    rdd.flatMap(tokenize)\n",
    "       .map(lambda w: (w, 1))\n",
    "       .reduceByKey(lambda a, b: a + b)\n",
    "       .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "word_counts.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca4d9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save results (local folder)\n",
    "out_dir = \"wordcount_out\"\n",
    "\n",
    "import shutil, os\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "word_counts.coalesce(1).saveAsTextFile(out_dir)\n",
    "print(\"Saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b096b25",
   "metadata": {},
   "source": [
    "# Part B — PySpark Lab (DataFrames): Cleaning + analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63090017",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_wc = word_counts.toDF([\"word\", \"count\"])\n",
    "df_wc.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b5716",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Top 10 words\n",
    "df_wc.orderBy(F.desc(\"count\")).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9982f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Add derived features\n",
    "total_words = df_wc.agg(F.sum(\"count\").alias(\"total\")).collect()[0][\"total\"]\n",
    "df_features = (df_wc\n",
    "               .withColumn(\"length\", F.length(\"word\"))\n",
    "               .withColumn(\"freq\", F.col(\"count\") / F.lit(total_words)))\n",
    "\n",
    "df_features.orderBy(F.desc(\"count\")).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd7540",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "df_features.createOrReplaceTempView(\"wc\")\n",
    "\n",
    "query = '''\n",
    "SELECT word, count, length, freq\n",
    "FROM wc\n",
    "WHERE length >= 6\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "'''\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d09b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Part C — MongoDB with PyMongo \n",
    "#Make sure MongoDB is running and Compass can connect to `mongodb://localhost:27017`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d3570",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "db = client[\"landmark_ai_lab\"]\n",
    "col = db[\"wordcount\"]\n",
    "\n",
    "# reset collection\n",
    "col.delete_many({})\n",
    "\n",
    "top50 = df_features.orderBy(F.desc(\"count\")).limit(50).toPandas()\n",
    "records = top50.to_dict(orient=\"records\")\n",
    "col.insert_many(records)\n",
    "\n",
    "print(\"Inserted documents:\", col.count_documents({}))\n",
    "records[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ca39c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Read back (PyMongo)\n",
    "list(col.find({}, {\"_id\": 0}).sort(\"count\", -1).limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc8b0e",
   "metadata": {},
   "source": [
    "# Part D — MongoDB Spark Connector \n",
    "This requires the connector JAR (via Maven package or local file).\n",
    "If it fails, students still get full marks using Part C with PyMongo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59aea9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# OPTIONAL: SparkSession configured for Mongo Spark Connector\n",
    "# spark.stop()\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = (SparkSession.builder\n",
    "#          .appName(\"Landmark-PySpark-Mongo-Connector\")\n",
    "#          .master(\"local[*]\")\n",
    "#          .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\")\n",
    "#          .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/landmark_ai_lab.wordcount\")\n",
    "#          .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/landmark_ai_lab.wordcount\")\n",
    "#          .getOrCreate())\n",
    "\n",
    "# df_mongo = spark.read.format(\"mongodb\").load()\n",
    "# df_mongo.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f389f6",
   "metadata": {},
   "source": [
    "# Exercises (lab) — to be completed \n",
    "## Exercise 1 (RDD)\n",
    "Using the same corpus:\n",
    "1. Remove stopwords: `{\"is\",\"a\",\"the\",\"to\",\"with\",\"and\"}`  \n",
    "2. Recompute word counts  \n",
    "3. Compare top 10 before vs after stopword removal\n",
    "\n",
    "## Exercise 2 (DataFrames) \n",
    "Create a DataFrame with columns: `word, count, length, freq` and:\n",
    "1. Compute average word length weighted by frequency  \n",
    "2. Return the 10 longest words and their counts  \n",
    "3. Filter words with `count >= 2` and show their share of total frequency\n",
    "\n",
    "## Exercise 3 (MongoDB + PyMongo) \n",
    "1. Store all words (not only top 50) into MongoDB  \n",
    "2. Create an index on `count` descending  \n",
    "3. Query: return words with `length >= 7` sorted by count\n",
    "\n",
    "## Exercise 4\n",
    "Download any public text dataset (or scrape a few articles), and build a Spark pipeline:\n",
    "- tokenization + cleaning  \n",
    "- word count  \n",
    "- top bigrams (pairs of consecutive words)  \n",
    "- store results in MongoDB, visualize in Compass\n",
    "\n",
    "## Exercise 5 \n",
    "Use Spark ML to compute TF‑IDF:\n",
    "- `pyspark.ml.feature.Tokenizer`, `HashingTF`, `IDF`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60096d",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "1) Build a Spark pipeline that reads **multiple text files** from a folder and produces:\n",
    "- per-file word count  \n",
    "- global word count  \n",
    "- top 20 keywords per file\n",
    "\n",
    "2) Extend the pipeline to store both results in MongoDB:\n",
    "- Collection 1: `global_wordcount`  \n",
    "- Collection 2: `per_file_wordcount`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fa8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
