{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22865edd",
   "metadata": {},
   "source": [
    "# Installation Tutorial &  Lab\n",
    "## Dr. Aurelle TCHAGNA\n",
    "# Course: Advanced AI / Big Data Lab (Landmark University, M.Tech)  \n",
    "**Goal:** Install and validate **Apache Spark**, **PySpark**, **MongoDB**, **PyMongo**, **MongoDB Spark Connector**, and **MongoDB Compass**, then practice PySpark with a **WordCount** pipeline and DataFrame analytics.\n",
    "\n",
    "This notebook contains:  \n",
    "1 installation steps (Windows/Linux/macOS)  \n",
    "2 verification commands  \n",
    "3 PySpark lab (RDD + DataFrame)  \n",
    "4 MongoDB operations (PyMongo + Spark Connector config)  \n",
    "5 exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a748b",
   "metadata": {},
   "source": [
    "## 0) What you will install\n",
    "### Required\n",
    "- **Java (JDK 11 or 17)**: Spark runs on the JVM.\n",
    "- **Apache Spark**: distributed compute engine.\n",
    "- **Python 3.9 to 3.11.14 (Higher versions have reported bugs for windows)**: for PySpark scripts and notebooks.\n",
    "- **PySpark**: Python API for Spark.\n",
    "- **MongoDB Community Server**: NoSQL database.\n",
    "- **PyMongo**: Python driver for MongoDB.\n",
    "- **MongoDB Compass**: GUI to view/edit MongoDB data.\n",
    "\n",
    "### Optional (Recommended)\n",
    "- **MongoDB Spark Connector**: enables Spark to read/write MongoDB efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073dd08",
   "metadata": {},
   "source": [
    "## 1) Install Java (JDK)\n",
    "### Windows\n",
    "1. Install **JDK 17** (or 11).\n",
    "2. Set environment variables:\n",
    "   - `JAVA_HOME=C:\\Program Files\\Java\\jdk-17`\n",
    "   - Add `%JAVA_HOME%\\bin` to `PATH`\n",
    "\n",
    "### Ubuntu/Debian\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install -y openjdk-17-jdk\n",
    "java -version\n",
    "```\n",
    "\n",
    "### macOS (Homebrew)\n",
    "```bash\n",
    "brew install openjdk@17\n",
    "java -version\n",
    "```\n",
    "\n",
    "✅ **Check**\n",
    "```bash\n",
    "java -version\n",
    "echo %JAVA_HOME%   # Windows\n",
    "echo $JAVA_HOME    # Linux/macOS\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470b4b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2) Install Apache Spark\n",
    "Download Spark (prebuilt for Hadoop) from the official Apache Spark site.\n",
    "\n",
    "**Recommendations for Windows Users**\n",
    "- Choose a Spark release: 3.5.8 (Jan 15 2026)\n",
    "- Choose a package type: Pre-built for Apache Hadoop 3.3 and later (Scala 2.13)\n",
    "\n",
    "After extracting, set:\n",
    "- `SPARK_HOME` to the Spark folder\n",
    "- Add `$SPARK_HOME/bin` to `PATH`\n",
    "\n",
    "**Windows example**\n",
    "- `SPARK_HOME=C:\\spark\\spark-3.5.1-bin-hadoop3`\n",
    "- Add `C:\\spark\\spark-3.5.1-bin-hadoop3\\bin` to PATH\n",
    "\n",
    "**Linux/macOS example**\n",
    "```bash\n",
    "export SPARK_HOME=$HOME/spark/spark-3.5.1-bin-hadoop3\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "✅ **Check**\n",
    "```bash\n",
    "spark-submit --version\n",
    "pyspark --version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2651a6",
   "metadata": {},
   "source": [
    "## 3) Install Python packages (PySpark + PyMongo)\n",
    "Create a virtual environment (recommended) then install packages.\n",
    "\n",
    "### Windows (PowerShell)\n",
    "```powershell\n",
    "python -m venv .venv\n",
    ".venv\\Scripts\\activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark==3.5.8 pymongo\n",
    "```\n",
    "\n",
    "### Linux/macOS\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark pymongo\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93be88",
   "metadata": {},
   "source": [
    "## 4) Install MongoDB Community Server + Compass\n",
    "### MongoDB Server\n",
    "Install MongoDB Community Server (Windows/macOS/Linux) from MongoDB official downloads.\n",
    "\n",
    "✅ After installation, ensure MongoDB is running.\n",
    "\n",
    "**Windows**\n",
    "- Open **Services** → start **MongoDB Server**  \n",
    "- Default URI: `mongodb://localhost:27017`\n",
    "\n",
    "**Linux (systemd)**\n",
    "```bash\n",
    "sudo systemctl start mongod\n",
    "sudo systemctl status mongod\n",
    "```\n",
    "\n",
    "### MongoDB Compass\n",
    "Install **MongoDB Compass** and connect using:\n",
    "- `mongodb://localhost:27017`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febedf3",
   "metadata": {},
   "source": [
    "## 6) Quick verification inside Jupyter\n",
    "Run the next cells. They check PySpark/PyMongo and start a local Spark session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583675a",
   "metadata": {},
   "source": [
    "## 5) MongoDB Spark Connector (important notes)\n",
    "Spark needs the MongoDB connector **JAR** when reading/writing MongoDB via Spark.\n",
    "\n",
    "### Option A: Use Maven coordinate (easy)\n",
    "In SparkSession config:\n",
    "- `org.mongodb.spark:mongo-spark-connector_2.12:10.3.0` (example)\n",
    "\n",
    "### Option B: Download connector JAR manually\n",
    "Provide it to Spark using `--jars` or `spark.jars`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1902355f-ae25-43cd-a19c-38269d238c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.5.1 findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4639e84-e927-49ba-a5f4-f6065b5edd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 17.0.17 2025-10-21\n",
      "OpenJDK Runtime Environment Temurin-17.0.17+10 (build 17.0.17+10)\n",
      "OpenJDK 64-Bit Server VM Temurin-17.0.17+10 (build 17.0.17+10, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b10a3cf-92fa-49d3-b208-46d85ca73ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.8\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee872f3c-64cb-41db-b1d3-b2955d701e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.16.0\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "print(pymongo.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "133dec70-5f7f-4bcd-9dc9-338248521f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6caa3-331c-42c2-9282-8ab33f68e2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f8be7-6abd-424d-a1eb-9ea5d0b5cb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4619a5b-42d0-4523-a083-20a98b94ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1497969-bd02-42a3-9d1c-4d6be9367336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22b4d356-7b66-400d-980b-6c120cc57a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark connected to MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoSparkExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.13:10.4.0\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/testdb.testcollection\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/testdb.testcollection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark connected to MongoDB successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c5f0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "PySpark version: 3.5.8\n",
      "PyMongo version: 4.16.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "    print(\"PySpark version:\", pyspark.__version__)\n",
    "except Exception as e:\n",
    "    print(\"PySpark error:\", e)\n",
    "\n",
    "try:\n",
    "    import pymongo\n",
    "    print(\"PyMongo version:\", pymongo.__version__)\n",
    "except Exception as e:\n",
    "    print(\"PyMongo error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88731ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-EE852R9:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Landmark-PySpark-Mongo-Connector</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24266927750>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Landmark-PySpark-Lab\")\n",
    "         .master(\"local[*]\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37afa05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.8'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433d53c",
   "metadata": {},
   "source": [
    "# Part A — PySpark Lab (RDD): WordCount \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "451b5297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark is fast. Spark is general-purpose.',\n",
       " 'PySpark lets you use Spark with Python.',\n",
       " 'Big data processing with Spark is scalable and efficient.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"Spark is fast. Spark is general-purpose.\",\n",
    "    \"PySpark lets you use Spark with Python.\",\n",
    "    \"Big data processing with Spark is scalable and efficient.\",\n",
    "    \"MongoDB is a NoSQL database. PyMongo connects Python to MongoDB.\"\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(text)\n",
    "rdd.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e84551f8-df6e-4ba6-9b19-9e4b3f73503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Spark is fast. Spark is general-purpose.\",\n",
    "    \"PySpark lets you use Spark with Python.\",\n",
    "    \"Big data processing with Spark is scalable and efficient.\",\n",
    "    \"MongoDB is a NoSQL database. PyMongo connects Python to MongoDB.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    rdd = spark.sparkContext.parallelize(text)\n",
    "    rdd.take(3)\n",
    "except Exception as err:\n",
    "    print(f\"Error parallelize: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55258d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 4),\n",
       " ('spark', 4),\n",
       " ('python', 2),\n",
       " ('mongodb', 2),\n",
       " ('with', 2),\n",
       " ('and', 1),\n",
       " ('pymongo', 1),\n",
       " ('pyspark', 1),\n",
       " ('general', 1),\n",
       " ('you', 1),\n",
       " ('connects', 1),\n",
       " ('processing', 1),\n",
       " ('a', 1),\n",
       " ('database', 1),\n",
       " ('fast', 1),\n",
       " ('lets', 1),\n",
       " ('use', 1),\n",
       " ('big', 1),\n",
       " ('to', 1),\n",
       " ('purpose', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(line: str):\n",
    "    return re.findall(r\"[a-z0-9]+\", line.lower())\n",
    "\n",
    "word_counts = (\n",
    "    rdd.flatMap(tokenize)\n",
    "       .map(lambda w: (w, 1))\n",
    "       .reduceByKey(lambda a, b: a + b)\n",
    "       .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "word_counts.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77ca4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: wordcount_out\n"
     ]
    }
   ],
   "source": [
    "# Save results (local folder)\n",
    "out_dir = \"wordcount_out\"\n",
    "\n",
    "import shutil, os\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "word_counts.coalesce(1).saveAsTextFile(out_dir)\n",
    "print(\"Saved to:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b096b25",
   "metadata": {},
   "source": [
    "# Part B — PySpark Lab (DataFrames): Cleaning + analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63090017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|     is|    4|\n",
      "|  spark|    4|\n",
      "| python|    2|\n",
      "|mongodb|    2|\n",
      "|   with|    2|\n",
      "|    and|    1|\n",
      "|pymongo|    1|\n",
      "|pyspark|    1|\n",
      "|general|    1|\n",
      "|    you|    1|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_wc = word_counts.toDF([\"word\", \"count\"])\n",
    "df_wc.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "726b5716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|     is|    4|\n",
      "|  spark|    4|\n",
      "| python|    2|\n",
      "|mongodb|    2|\n",
      "|   with|    2|\n",
      "|    and|    1|\n",
      "|pymongo|    1|\n",
      "|pyspark|    1|\n",
      "|general|    1|\n",
      "|    you|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words\n",
    "df_wc.orderBy(F.desc(\"count\")).limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9982f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+--------------------+\n",
      "|   word|count|length|                freq|\n",
      "+-------+-----+------+--------------------+\n",
      "|     is|    4|     2| 0.12121212121212122|\n",
      "|  spark|    4|     5| 0.12121212121212122|\n",
      "| python|    2|     6| 0.06060606060606061|\n",
      "|mongodb|    2|     7| 0.06060606060606061|\n",
      "|   with|    2|     4| 0.06060606060606061|\n",
      "|    and|    1|     3|0.030303030303030304|\n",
      "|pymongo|    1|     7|0.030303030303030304|\n",
      "|pyspark|    1|     7|0.030303030303030304|\n",
      "|general|    1|     7|0.030303030303030304|\n",
      "|    you|    1|     3|0.030303030303030304|\n",
      "+-------+-----+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add derived features\n",
    "total_words = df_wc.agg(F.sum(\"count\").alias(\"total\")).collect()[0][\"total\"]\n",
    "df_features = (df_wc\n",
    "               .withColumn(\"length\", F.length(\"word\"))\n",
    "               .withColumn(\"freq\", F.col(\"count\") / F.lit(total_words)))\n",
    "\n",
    "df_features.orderBy(F.desc(\"count\")).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75cd7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+--------------------+\n",
      "|      word|count|length|                freq|\n",
      "+----------+-----+------+--------------------+\n",
      "|    python|    2|     6| 0.06060606060606061|\n",
      "|   mongodb|    2|     7| 0.06060606060606061|\n",
      "|   pymongo|    1|     7|0.030303030303030304|\n",
      "|   pyspark|    1|     7|0.030303030303030304|\n",
      "|   general|    1|     7|0.030303030303030304|\n",
      "|  connects|    1|     8|0.030303030303030304|\n",
      "|processing|    1|    10|0.030303030303030304|\n",
      "|  database|    1|     8|0.030303030303030304|\n",
      "|   purpose|    1|     7|0.030303030303030304|\n",
      "| efficient|    1|     9|0.030303030303030304|\n",
      "+----------+-----+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "df_features.createOrReplaceTempView(\"wc\")\n",
    "\n",
    "query = '''\n",
    "SELECT word, count, length, freq\n",
    "FROM wc\n",
    "WHERE length >= 6\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "'''\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d09b7",
   "metadata": {},
   "source": [
    "# Part C — MongoDB with PyMongo \n",
    "Make sure MongoDB is running and Compass can connect to `mongodb://localhost:27017`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec4d3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted documents: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'is',\n",
       "  'count': 4,\n",
       "  'length': 2,\n",
       "  'freq': 0.12121212121212122,\n",
       "  '_id': ObjectId('697c4f7089189cc886d65374')},\n",
       " {'word': 'spark',\n",
       "  'count': 4,\n",
       "  'length': 5,\n",
       "  'freq': 0.12121212121212122,\n",
       "  '_id': ObjectId('697c4f7089189cc886d65375')}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "db = client[\"landmark_ai_lab\"]\n",
    "col = db[\"wordcount\"]\n",
    "\n",
    "# reset collection\n",
    "col.delete_many({})\n",
    "\n",
    "top50 = df_features.orderBy(F.desc(\"count\")).limit(50).toPandas()\n",
    "records = top50.to_dict(orient=\"records\")\n",
    "col.insert_many(records)\n",
    "\n",
    "print(\"Inserted documents:\", col.count_documents({}))\n",
    "records[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc3ca39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'is', 'count': 4, 'length': 2, 'freq': 0.12121212121212122},\n",
       " {'word': 'spark', 'count': 4, 'length': 5, 'freq': 0.12121212121212122},\n",
       " {'word': 'mongodb', 'count': 2, 'length': 7, 'freq': 0.06060606060606061},\n",
       " {'word': 'with', 'count': 2, 'length': 4, 'freq': 0.06060606060606061},\n",
       " {'word': 'python', 'count': 2, 'length': 6, 'freq': 0.06060606060606061},\n",
       " {'word': 'pyspark', 'count': 1, 'length': 7, 'freq': 0.030303030303030304},\n",
       " {'word': 'general', 'count': 1, 'length': 7, 'freq': 0.030303030303030304},\n",
       " {'word': 'you', 'count': 1, 'length': 3, 'freq': 0.030303030303030304},\n",
       " {'word': 'and', 'count': 1, 'length': 3, 'freq': 0.030303030303030304},\n",
       " {'word': 'pymongo', 'count': 1, 'length': 7, 'freq': 0.030303030303030304}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read back (PyMongo)\n",
    "list(col.find({}, {\"_id\": 0}).sort(\"count\", -1).limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc8b0e",
   "metadata": {},
   "source": [
    "# Part D — MongoDB Spark Connector \n",
    "This requires the connector JAR (via Maven package or local file).\n",
    "If it fails, students still get full marks using Part C with PyMongo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd59aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------+-------+\n",
      "|                 _id|count|                freq|length|   word|\n",
      "+--------------------+-----+--------------------+------+-------+\n",
      "|697c4f7089189cc88...|    4| 0.12121212121212122|     2|     is|\n",
      "|697c4f7089189cc88...|    4| 0.12121212121212122|     5|  spark|\n",
      "|697c4f7089189cc88...|    2| 0.06060606060606061|     6| python|\n",
      "|697c4f7089189cc88...|    2| 0.06060606060606061|     7|mongodb|\n",
      "|697c4f7089189cc88...|    2| 0.06060606060606061|     4|   with|\n",
      "|697c4f7089189cc88...|    1|0.030303030303030304|     3|    and|\n",
      "|697c4f7089189cc88...|    1|0.030303030303030304|     7|pymongo|\n",
      "|697c4f7089189cc88...|    1|0.030303030303030304|     7|pyspark|\n",
      "|697c4f7089189cc88...|    1|0.030303030303030304|     7|general|\n",
      "|697c4f7089189cc88...|    1|0.030303030303030304|     3|    you|\n",
      "+--------------------+-----+--------------------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: SparkSession configured for Mongo Spark Connector\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Landmark-PySpark-Mongo-Connector\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.13:10.4.0\")\n",
    "         .config(\"spark.mongodb.read.connection.uri\", \"mongodb://localhost:27017/landmark_ai_lab.wordcount\")\n",
    "         .config(\"spark.mongodb.write.connection.uri\", \"mongodb://localhost:27017/landmark_ai_lab.wordcount\")\n",
    "         .getOrCreate())\n",
    "\n",
    "df_mongo = spark.read.format(\"mongodb\").load()\n",
    "df_mongo.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5954457-f1f8-4e16-b6a8-05c5855f5a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11937210-9ee4-4ee3-a9ef-c2aa4adf0669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee81fd-e16c-4e7a-89dc-24de3c0ca92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f389f6",
   "metadata": {},
   "source": [
    "# Exercises (lab) — to be completed \n",
    "## Exercise 1 (RDD)\n",
    "Using the same corpus:\n",
    "1. Remove stopwords: `{\"is\",\"a\",\"the\",\"to\",\"with\",\"and\"}`  \n",
    "2. Recompute word counts  \n",
    "3. Compare top 10 before vs after stopword removal\n",
    "\n",
    "## Exercise 2 (DataFrames) \n",
    "Create a DataFrame with columns: `word, count, length, freq` and:\n",
    "1. Compute average word length weighted by frequency  \n",
    "2. Return the 10 longest words and their counts  \n",
    "3. Filter words with `count >= 2` and show their share of total frequency\n",
    "\n",
    "## Exercise 3 (MongoDB + PyMongo) \n",
    "1. Store all words (not only top 50) into MongoDB  \n",
    "2. Create an index on `count` descending  \n",
    "3. Query: return words with `length >= 7` sorted by count\n",
    "\n",
    "## Exercise 4\n",
    "Download any public text dataset (or scrape a few articles), and build a Spark pipeline:\n",
    "- tokenization + cleaning  \n",
    "- word count  \n",
    "- top bigrams (pairs of consecutive words)  \n",
    "- store results in MongoDB, visualize in Compass\n",
    "\n",
    "## Exercise 5 \n",
    "Use Spark ML to compute TF‑IDF:\n",
    "- `pyspark.ml.feature.Tokenizer`, `HashingTF`, `IDF`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5f80fea-7dad-490a-81d8-39a079b6bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 AFTER STOPWORD REMOVAL:\n",
      "spark: 4\n",
      "python: 2\n",
      "mongodb: 2\n",
      "pymongo: 1\n",
      "pyspark: 1\n",
      "general: 1\n",
      "you: 1\n",
      "connects: 1\n",
      "processing: 1\n",
      "database: 1\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 1: COMPLETE FIX ---\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Restart the Spark Engine (Get a fresh session)\n",
    "# This fixes the \"NoneType\" error by reconnecting the engine\n",
    "spark = SparkSession.builder.appName(\"Exercise1\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# 2. Re-create the input data\n",
    "text = [\n",
    "    \"Spark is fast. Spark is general-purpose.\",\n",
    "    \"PySpark lets you use Spark with Python.\",\n",
    "    \"Big data processing with Spark is scalable and efficient.\",\n",
    "    \"MongoDB is a NoSQL database. PyMongo connects Python to MongoDB.\"\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(text)\n",
    "\n",
    "# 3. Define the helper function and stopwords\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[a-z]+', text.lower())\n",
    "\n",
    "stopwords = {\"is\", \"a\", \"the\", \"to\", \"with\", \"and\"}\n",
    "\n",
    "# 4. Run the Pipeline: Tokenize -> Filter -> Count\n",
    "word_counts_filtered = (\n",
    "    rdd.flatMap(tokenize)\n",
    "       .filter(lambda word: word not in stopwords) # <--- The Filter Step\n",
    "       .map(lambda word: (word, 1))\n",
    "       .reduceByKey(lambda a, b: a + b)\n",
    "       .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "# 5. Print Result\n",
    "print(\"TOP 10 AFTER STOPWORD REMOVAL:\")\n",
    "for word, count in word_counts_filtered.take(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e006d-ab1c-4964-b0ec-934746b562e2",
   "metadata": {},
   "source": [
    "# Solution To Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d18e168d-bffd-49ff-874e-5e01f87c79d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The DataFrame Structure ---\n",
      "+-------+-----+------+--------------------+\n",
      "|   word|count|length|                freq|\n",
      "+-------+-----+------+--------------------+\n",
      "| python|    2|     6| 0.06060606060606061|\n",
      "|    and|    1|     3|0.030303030303030304|\n",
      "|pymongo|    1|     7|0.030303030303030304|\n",
      "|     is|    4|     2| 0.12121212121212122|\n",
      "|pyspark|    1|     7|0.030303030303030304|\n",
      "+-------+-----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "1. Average Word Length (Weighted): 5.0000 characters\n",
      "\n",
      "--- 2. Top 10 Longest Words ---\n",
      "+----------+-----+------+\n",
      "|      word|count|length|\n",
      "+----------+-----+------+\n",
      "|processing|    1|    10|\n",
      "| efficient|    1|     9|\n",
      "|  scalable|    1|     8|\n",
      "|  database|    1|     8|\n",
      "|  connects|    1|     8|\n",
      "|   pymongo|    1|     7|\n",
      "|   mongodb|    2|     7|\n",
      "|   purpose|    1|     7|\n",
      "|   general|    1|     7|\n",
      "|   pyspark|    1|     7|\n",
      "+----------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3. Share of total frequency for words with count >= 2: 42.42%\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 2: COMPLETE SOLUTION ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "# 1. SETUP: Restart Spark and Re-create Data (To avoid errors)\n",
    "spark = SparkSession.builder.appName(\"Exercise2\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "text = [\n",
    "    \"Spark is fast. Spark is general-purpose.\",\n",
    "    \"PySpark lets you use Spark with Python.\",\n",
    "    \"Big data processing with Spark is scalable and efficient.\",\n",
    "    \"MongoDB is a NoSQL database. PyMongo connects Python to MongoDB.\"\n",
    "]\n",
    "\n",
    "# Quick RDD pipeline to get word counts (from Exercise 1)\n",
    "rdd = spark.sparkContext.parallelize(text)\n",
    "def tokenize(t): return re.findall(r'[a-z]+', t.lower())\n",
    "word_counts_rdd = rdd.flatMap(tokenize).map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 0: Create the DataFrame with columns: word, count, length, freq\n",
    "# ---------------------------------------------------------\n",
    "# Convert RDD to DataFrame\n",
    "df = word_counts_rdd.toDF([\"word\", \"count\"])\n",
    "\n",
    "# Calculate Total Words (needed for frequency)\n",
    "total_words = df.agg(F.sum(\"count\")).collect()[0][0]\n",
    "\n",
    "# Add 'length' and 'freq' columns\n",
    "df_features = (\n",
    "    df.withColumn(\"length\", F.length(\"word\"))\n",
    "      .withColumn(\"freq\", F.col(\"count\") / F.lit(total_words))\n",
    ")\n",
    "\n",
    "print(\"--- The DataFrame Structure ---\")\n",
    "df_features.show(5)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# QUESTION 1: Compute average word length weighted by frequency\n",
    "# ---------------------------------------------------------\n",
    "# Logic: If 'is' (len 2) appears 4 times and 'general' (len 7) appears 1 time,\n",
    "# we shouldn't just average 2 and 7. We must weight them by how often they appear.\n",
    "# Formula: Sum(length * freq)\n",
    "avg_weighted_length = df_features.agg(F.sum(F.col(\"length\") * F.col(\"freq\"))).collect()[0][0]\n",
    "\n",
    "print(f\"1. Average Word Length (Weighted): {avg_weighted_length:.4f} characters\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# QUESTION 2: Return the 10 longest words and their counts\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- 2. Top 10 Longest Words ---\")\n",
    "(df_features\n",
    "    .orderBy(F.desc(\"length\")) # Sort by length descending\n",
    "    .select(\"word\", \"count\", \"length\")\n",
    "    .show(10)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# QUESTION 3: Filter words with count >= 2 and show share of total frequency\n",
    "# ---------------------------------------------------------\n",
    "# Logic: We find all words that appear at least twice, and sum up their frequencies.\n",
    "freq_share = (\n",
    "    df_features\n",
    "    .filter(F.col(\"count\") >= 2)\n",
    "    .agg(F.sum(\"freq\"))\n",
    "    .collect()[0][0]\n",
    ")\n",
    "\n",
    "print(f\"3. Share of total frequency for words with count >= 2: {freq_share:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a6408-3274-4107-b8ff-03655ce95107",
   "metadata": {},
   "source": [
    "# Solution To Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffc3ef64-1cec-48b1-8fb2-8cee153e7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Successfully stored 24 words in MongoDB.\n",
      "2. Created descending index on the 'count' field.\n",
      "\n",
      "3. Words with length >= 7 (Sorted by count):\n",
      "{'word': 'mongodb', 'count': 2, 'length': 7, 'freq': 0.06060606060606061}\n",
      "{'word': 'pymongo', 'count': 1, 'length': 7, 'freq': 0.030303030303030304}\n",
      "{'word': 'pyspark', 'count': 1, 'length': 7, 'freq': 0.030303030303030304}\n",
      "{'word': 'general', 'count': 1, 'length': 7, 'freq': 0.030303030303030304}\n",
      "{'word': 'connects', 'count': 1, 'length': 8, 'freq': 0.030303030303030304}\n",
      "{'word': 'processing', 'count': 1, 'length': 10, 'freq': 0.030303030303030304}\n",
      "{'word': 'database', 'count': 1, 'length': 8, 'freq': 0.030303030303030304}\n",
      "{'word': 'purpose', 'count': 1, 'length': 7, 'freq': 0.030303030303030304}\n",
      "{'word': 'efficient', 'count': 1, 'length': 9, 'freq': 0.030303030303030304}\n",
      "{'word': 'scalable', 'count': 1, 'length': 8, 'freq': 0.030303030303030304}\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 3: MONGODB + PYMONGO ---\n",
    "from pymongo import MongoClient, DESCENDING\n",
    "\n",
    "# 1. Connect to MongoDB\n",
    "# Ensure your MongoDB Server is running as per Part C instructions\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"landmark_ai_lab\"]\n",
    "col = db[\"full_wordcount\"]\n",
    "\n",
    "# Clear existing data so we start fresh\n",
    "col.delete_many({})\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 1: Store ALL words (not only top 50)\n",
    "# ---------------------------------------------------------\n",
    "# Convert the entire Spark DataFrame to a list of dictionaries for MongoDB\n",
    "all_records = df_features.toPandas().to_dict(orient=\"records\")\n",
    "col.insert_many(all_records)\n",
    "\n",
    "print(f\"1. Successfully stored {col.count_documents({})} words in MongoDB.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 2: Create an index on 'count' descending\n",
    "# ---------------------------------------------------------\n",
    "# This makes searching and sorting by count much faster\n",
    "col.create_index([(\"count\", DESCENDING)])\n",
    "print(\"2. Created descending index on the 'count' field.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP 3: Query words with length >= 7 sorted by count\n",
    "# ---------------------------------------------------------\n",
    "# We use a filter: {\"length\": {\"$gte\": 7}} means length >= 7\n",
    "query_filter = {\"length\": {\"$gte\": 7}}\n",
    "results = col.find(query_filter, {\"_id\": 0}).sort(\"count\", DESCENDING)\n",
    "\n",
    "print(\"\\n3. Words with length >= 7 (Sorted by count):\")\n",
    "for doc in results:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76dc0ee-472d-42c3-965a-d41af419ed99",
   "metadata": {},
   "source": [
    "#### Solution To Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4395c1d7-fc77-429b-9c40-23aef07cb9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOP 10 BIGRAMS ---\n",
      "+------------+-----+\n",
      "|      bigram|count|\n",
      "+------------+-----+\n",
      "|            |  134|\n",
      "|      of his|    8|\n",
      "|    that you|    7|\n",
      "|adventure of|    6|\n",
      "|    you have|    6|\n",
      "|   adventure|    6|\n",
      "|         of |    6|\n",
      "|          i |    5|\n",
      "|      he was|    5|\n",
      "|      do you|    4|\n",
      "+------------+-----+\n",
      "\n",
      "Pipeline Complete! Saved 10 bigrams to 'top_bigrams' collection.\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 4: COMPLETE PIPELINE ---\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 1. SETUP: Start Spark and MongoDB connection\n",
    "spark = SparkSession.builder.appName(\"Exercise4_Pipeline\").master(\"local[*]\").getOrCreate()\n",
    "client = MongoClient(\"mongodb://localhost:27017\") #\n",
    "db = client[\"landmark_ai_lab\"] #\n",
    "\n",
    "# 2. DOWNLOAD DATA: Fetching a public text dataset (Sherlock Holmes)\n",
    "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw_text = response.read().decode('utf-8')[1000:10000] # Taking a slice for speed\n",
    "\n",
    "# 3. CLEANING PIPELINE: Create a DataFrame and clean the text\n",
    "# Create a single-row DataFrame with the text\n",
    "data = spark.createDataFrame([(raw_text,)], [\"raw_content\"])\n",
    "\n",
    "# Step A: Tokenization (Break text into words)\n",
    "tokenizer = Tokenizer(inputCol=\"raw_content\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(data)\n",
    "\n",
    "# Step B: Cleaning (Remove the lab's specific stopwords + standard ones)\n",
    "lab_stopwords = [\"is\", \"a\", \"the\", \"to\", \"with\", \"and\"] #\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=lab_stopwords)\n",
    "clean_df = remover.transform(words_df)\n",
    "\n",
    "# 4. BIGRAM GENERATION: Create pairs of words\n",
    "ngram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"bigrams\")\n",
    "bigram_df = ngram.transform(clean_df)\n",
    "\n",
    "# 5. ANALYSIS: Count the top 10 bigrams\n",
    "# Explode the list of bigrams into individual rows to count them\n",
    "top_bigrams = (\n",
    "    bigram_df.select(F.explode(\"bigrams\").alias(\"bigram\"))\n",
    "    .groupBy(\"bigram\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "print(\"--- TOP 10 BIGRAMS ---\")\n",
    "top_bigrams.show()\n",
    "\n",
    "# 6. STORAGE: Save results in MongoDB\n",
    "# Convert to list and insert\n",
    "bigram_records = top_bigrams.toPandas().to_dict(orient=\"records\")\n",
    "db[\"top_bigrams\"].delete_many({}) # Clear old results\n",
    "db[\"top_bigrams\"].insert_many(bigram_records)\n",
    "\n",
    "print(f\"Pipeline Complete! Saved {len(bigram_records)} bigrams to 'top_bigrams' collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991b7d3-8c23-4a00-96df-3fddcd6d39a4",
   "metadata": {},
   "source": [
    "#Solution To Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18e12e41-9f9a-400b-a984-6aa2618352a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features (Word Importance):\n",
      "+--------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentence                                                      |features                                                                                                                                         |\n",
      "+--------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Spark is fast Spark is general-purpose                        |(20,[0,3,6,9],[0.9162907318741551,0.9162907318741551,0.44628710262841953,0.0])                                                                   |\n",
      "|PySpark lets you use Spark with Python                        |(20,[1,6,9,10,13,17,18],[0.5108256237659907,0.22314355131420976,0.0,0.5108256237659907,0.9162907318741551,0.9162907318741551,0.5108256237659907])|\n",
      "|Big data processing with Spark is scalable and efficient      |(20,[6,8,9,10,11,15],[0.6694306539426294,0.5108256237659907,0.0,1.0216512475319814,0.9162907318741551,0.9162907318741551])                       |\n",
      "|MongoDB is a NoSQL database PyMongo connects Python to MongoDB|(20,[1,4,7,8,9,16,18],[0.5108256237659907,0.9162907318741551,0.9162907318741551,0.5108256237659907,0.0,1.8325814637483102,0.5108256237659907])   |\n",
      "+--------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 5: TF-IDF WITH SPARK ML ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "# 1. Setup Spark Session\n",
    "spark = SparkSession.builder.appName(\"Exercise5_TFIDF\").getOrCreate()\n",
    "\n",
    "# 2. Create the data (using the same corpus from previous exercises)\n",
    "sentence_data = spark.createDataFrame([\n",
    "    (0, \"Spark is fast Spark is general-purpose\"),\n",
    "    (1, \"PySpark lets you use Spark with Python\"),\n",
    "    (2, \"Big data processing with Spark is scalable and efficient\"),\n",
    "    (3, \"MongoDB is a NoSQL database PyMongo connects Python to MongoDB\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# STEP A: Tokenization\n",
    "# Converts the sentence into a list of words\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(sentence_data)\n",
    "\n",
    "# STEP B: HashingTF (Term Frequency)\n",
    "# numFeatures=20 means we map words into 20 possible numeric \"buckets\"\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurized_data = hashingTF.transform(words_data)\n",
    "\n",
    "# STEP C: IDF (Inverse Document Frequency)\n",
    "# This calculates the rarity of words across all 4 sentences\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "# 3. Show the Results\n",
    "print(\"TF-IDF Features (Word Importance):\")\n",
    "rescaled_data.select(\"sentence\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f264b6-527b-4cd6-9476-0b7baf45406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f60096d",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "1) Build a Spark pipeline that reads **multiple text files** from a folder and produces:\n",
    "- per-file word count  \n",
    "- global word count  \n",
    "- top 20 keywords per file\n",
    "\n",
    "2) Extend the pipeline to store both results in MongoDB:\n",
    "- Collection 1: `global_wordcount`  \n",
    "- Collection 2: `per_file_wordcount`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7150b1ee-7395-455b-b033-aafcc7ecaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-File Word Count ---\n",
      "+---------+------+-----+\n",
      "|     file|  word|count|\n",
      "+---------+------+-----+\n",
      "|file1.txt| apple|    2|\n",
      "|file1.txt|orange|    1|\n",
      "|file1.txt|banana|    1|\n",
      "|file2.txt|banana|    2|\n",
      "|file2.txt|cherry|    1|\n",
      "|file2.txt| apple|    1|\n",
      "|file3.txt|cherry|    2|\n",
      "|file3.txt| mango|    1|\n",
      "|file3.txt| apple|    1|\n",
      "+---------+------+-----+\n",
      "\n",
      "--- Global Word Count ---\n",
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "| apple|    4|\n",
      "|cherry|    3|\n",
      "|banana|    3|\n",
      "|orange|    1|\n",
      "| mango|    1|\n",
      "+------+-----+\n",
      "\n",
      "--- Top 20 Keywords Per File ---\n",
      "+---------+------+-----+\n",
      "|     file|  word|count|\n",
      "+---------+------+-----+\n",
      "|file1.txt| apple|    2|\n",
      "|file1.txt|orange|    1|\n",
      "|file1.txt|banana|    1|\n",
      "|file2.txt|banana|    2|\n",
      "|file2.txt|cherry|    1|\n",
      "|file2.txt| apple|    1|\n",
      "|file3.txt|cherry|    2|\n",
      "|file3.txt| mango|    1|\n",
      "|file3.txt| apple|    1|\n",
      "+---------+------+-----+\n",
      "\n",
      "\n",
      "MISSION COMPLETE: Data stored in MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# --- EXERCISE 6: MULTI-FILE PIPELINE ---\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 1. PREPARATION: Create a folder with 3 sample files\n",
    "os.makedirs(\"my_corpus\", exist_ok=True)\n",
    "with open(\"my_corpus/file1.txt\", \"w\") as f: f.write(\"apple orange apple banana\")\n",
    "with open(\"my_corpus/file2.txt\", \"w\") as f: f.write(\"banana cherry banana apple\")\n",
    "with open(\"my_corpus/file3.txt\", \"w\") as f: f.write(\"cherry cherry apple mango\")\n",
    "\n",
    "# 2. SETUP: Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Exercise6\").getOrCreate()\n",
    "\n",
    "# 3. READ MULTIPLE FILES: Use wholeTextFiles to get (FilePath, Content)\n",
    "raw_rdd = spark.sparkContext.wholeTextFiles(\"my_corpus\")\n",
    "\n",
    "# 4. PROCESSING: Create the \"Per-File Word Count\"\n",
    "def process_files(path_content):\n",
    "    path, content = path_content\n",
    "    filename = os.path.basename(path) # Get just 'file1.txt' instead of full path\n",
    "    words = content.lower().split()\n",
    "    return [(filename, word) for word in words]\n",
    "\n",
    "# This creates an RDD of (filename, word)\n",
    "per_file_words = raw_rdd.flatMap(process_files)\n",
    "\n",
    "# Convert to DataFrame for easier math\n",
    "# Schema: [file, word]\n",
    "df_all = per_file_words.toDF([\"file\", \"word\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GOAL A: Per-File Word Count\n",
    "# ---------------------------------------------------------\n",
    "per_file_count_df = df_all.groupBy(\"file\", \"word\").count().orderBy(\"file\", F.desc(\"count\"))\n",
    "print(\"--- Per-File Word Count ---\")\n",
    "per_file_count_df.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GOAL B: Global Word Count\n",
    "# ---------------------------------------------------------\n",
    "global_wordcount_df = df_all.groupBy(\"word\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"--- Global Word Count ---\")\n",
    "global_wordcount_df.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GOAL C: Top 20 Keywords Per File\n",
    "# ---------------------------------------------------------\n",
    "# We use a Window function to \"rank\" words inside each file\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window.partitionBy(\"file\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "top_20_per_file = (\n",
    "    per_file_count_df\n",
    "    .withColumn(\"rank\", F.row_number().over(windowSpec))\n",
    "    .filter(F.col(\"rank\") <= 20)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "print(\"--- Top 20 Keywords Per File ---\")\n",
    "top_20_per_file.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STORAGE: Store in MongoDB Collections\n",
    "# ---------------------------------------------------------\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"landmark_ai_lab\"]\n",
    "\n",
    "# Collection 1: Global Wordcount\n",
    "db[\"global_wordcount\"].delete_many({})\n",
    "db[\"global_wordcount\"].insert_many(global_wordcount_df.toPandas().to_dict(orient=\"records\"))\n",
    "\n",
    "# Collection 2: Per-File Wordcount\n",
    "db[\"per_file_wordcount\"].delete_many({})\n",
    "db[\"per_file_wordcount\"].insert_many(top_20_per_file.toPandas().to_dict(orient=\"records\"))\n",
    "\n",
    "print(\"\\nMISSION COMPLETE: Data stored in MongoDB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fa8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
